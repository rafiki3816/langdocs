"""
Streamlit ë°ëª¨ ì• í”Œë¦¬ì¼€ì´ì…˜ - ê°„ë‹¨í•œ ë²„ì „
LangChain RAG ì±—ë´‡ì˜ ì›¹ ì¸í„°í˜ì´ìŠ¤
"""

import streamlit as st
import os
from datetime import datetime
import time

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
from dotenv import load_dotenv
load_dotenv()

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(
    page_title="LangChain RAG ì±—ë´‡",
    page_icon="ğŸ¤–",
    layout="wide",
    initial_sidebar_state="expanded"
)

# CSS ìŠ¤íƒ€ì¼ ì¶”ê°€
st.markdown("""
<style>
    .stChat {
        padding: 20px;
    }
    .user-message {
        background-color: #E8F4FD;
        padding: 10px;
        border-radius: 10px;
        margin: 5px 0;
    }
    .assistant-message {
        background-color: #F0F0F0;
        padding: 10px;
        border-radius: 10px;
        margin: 5px 0;
    }
</style>
""", unsafe_allow_html=True)

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if 'messages' not in st.session_state:
    st.session_state.messages = []
if 'conversation_id' not in st.session_state:
    st.session_state.conversation_id = f"conv_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
if 'llm' not in st.session_state:
    st.session_state.llm = None
if 'retriever' not in st.session_state:
    st.session_state.retriever = None
if 'vector_db' not in st.session_state:
    st.session_state.vector_db = None

# ì‚¬ì´ë“œë°” ì„¤ì •
with st.sidebar:
    st.title("âš™ï¸ ì„¤ì •")

    # ì‹œìŠ¤í…œ ìƒíƒœ
    st.markdown("### ğŸ“Š ì‹œìŠ¤í…œ ìƒíƒœ")

    col1, col2 = st.columns(2)
    with col1:
        if st.button("ğŸ”„ ì‹œìŠ¤í…œ ì´ˆê¸°í™”"):
            with st.spinner("ì´ˆê¸°í™” ì¤‘..."):
                try:
                    from llm import get_llm
                    from vector_database import VectorDatabase
                    from retriever import HybridRetriever

                    # LLM ì´ˆê¸°í™”
                    st.session_state.llm = get_llm()

                    # ë²¡í„° DB ì´ˆê¸°í™”
                    st.session_state.vector_db = VectorDatabase()
                    st.session_state.vector_db.init_vectorstore()

                    # Retriever ì´ˆê¸°í™”
                    st.session_state.retriever = HybridRetriever(st.session_state.vector_db)

                    st.success("âœ… ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ!")
                except Exception as e:
                    st.error(f"âŒ ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}")

    with col2:
        if st.button("ğŸ—‘ï¸ ëŒ€í™” ì´ˆê¸°í™”"):
            st.session_state.messages = []
            st.session_state.conversation_id = f"conv_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
            st.success("âœ… ëŒ€í™” ì´ˆê¸°í™” ì™„ë£Œ!")

    # ì‹œìŠ¤í…œ ì •ë³´
    st.markdown("---")
    st.markdown("### ğŸ“ˆ ì‹œìŠ¤í…œ ì •ë³´")

    if st.session_state.llm:
        st.success("âœ… LLM ì—°ê²°ë¨")
    else:
        st.warning("âš ï¸ LLM ë¯¸ì—°ê²°")

    if st.session_state.vector_db:
        st.success("âœ… ë²¡í„° DB ì—°ê²°ë¨")
    else:
        st.warning("âš ï¸ ë²¡í„° DB ë¯¸ì—°ê²°")

    if st.session_state.retriever:
        st.success("âœ… ê²€ìƒ‰ ì‹œìŠ¤í…œ ì¤€ë¹„ë¨")
    else:
        st.warning("âš ï¸ ê²€ìƒ‰ ì‹œìŠ¤í…œ ë¯¸ì¤€ë¹„")

    # ê²€ìƒ‰ ì„¤ì •
    st.markdown("---")
    st.markdown("### ğŸ” ê²€ìƒ‰ ì„¤ì •")

    search_k = st.slider(
        "ê²€ìƒ‰í•  ë¬¸ì„œ ìˆ˜",
        min_value=1,
        max_value=10,
        value=3,
        help="ë” ë§ì€ ë¬¸ì„œë¥¼ ê²€ìƒ‰í•˜ë©´ ì •í™•ë„ê°€ ë†’ì•„ì§€ì§€ë§Œ ì†ë„ê°€ ëŠë ¤ì§‘ë‹ˆë‹¤."
    )

    temperature = st.slider(
        "ì°½ì˜ì„± (Temperature)",
        min_value=0.0,
        max_value=1.0,
        value=0.7,
        step=0.1,
        help="0ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ì¼ê´€ì„± ìˆê³ , 1ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ì°½ì˜ì ì…ë‹ˆë‹¤."
    )

    # ëŒ€í™” í†µê³„
    st.markdown("---")
    st.markdown("### ğŸ“Š ëŒ€í™” í†µê³„")
    st.info(f"ëŒ€í™” ID: {st.session_state.conversation_id[:20]}...")
    st.info(f"ë©”ì‹œì§€ ìˆ˜: {len(st.session_state.messages)}")

    # ì •ë³´
    st.markdown("---")
    st.markdown("### â„¹ï¸ ì •ë³´")
    st.markdown("""
    **LangChain RAG ì±—ë´‡ v1.0**

    - ğŸ¤– LLM: Upstage Solar
    - ğŸ’¾ Vector DB: ChromaDB
    - ğŸ“š ë¬¸ì„œ: LangChain Docs

    [GitHub](https://github.com) | [ë¬¸ì„œ](https://docs.example.com)
    """)

# ë©”ì¸ í™”ë©´
st.title("ğŸ¤– LangChain RAG ì±—ë´‡")
st.markdown("LangChain ë¬¸ì„œ ê¸°ë°˜ ì§€ëŠ¥í˜• Q&A ì‹œìŠ¤í…œ")

# íƒ­ ìƒì„±
tab1, tab2, tab3 = st.tabs(["ğŸ’¬ ì±„íŒ…", "ğŸ“š ë¬¸ì„œ ê´€ë¦¬", "ğŸ“ˆ í†µê³„"])

with tab1:
    # ì±„íŒ… ì¸í„°í˜ì´ìŠ¤

    # ì‹œìŠ¤í…œ ì²´í¬
    if not st.session_state.llm:
        st.warning("âš ï¸ ì‹œìŠ¤í…œì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì‚¬ì´ë“œë°”ì—ì„œ 'ì‹œìŠ¤í…œ ì´ˆê¸°í™”' ë²„íŠ¼ì„ í´ë¦­í•˜ì„¸ìš”.")

    # ëŒ€í™” ê¸°ë¡ í‘œì‹œ
    chat_container = st.container()
    with chat_container:
        for message in st.session_state.messages:
            with st.chat_message(message["role"]):
                st.markdown(message["content"])

    # ì…ë ¥ ì°½
    if prompt := st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”..."):
        if not st.session_state.llm:
            st.error("âŒ ë¨¼ì € ì‹œìŠ¤í…œì„ ì´ˆê¸°í™”í•´ì£¼ì„¸ìš”!")
        else:
            # ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€
            st.session_state.messages.append({"role": "user", "content": prompt})

            # ì‚¬ìš©ì ë©”ì‹œì§€ í‘œì‹œ
            with st.chat_message("user"):
                st.markdown(prompt)

            # ì–´ì‹œìŠ¤í„´íŠ¸ ì‘ë‹µ ìƒì„±
            with st.chat_message("assistant"):
                with st.spinner("ìƒê° ì¤‘..."):
                    try:
                        # ë¬¸ì„œ ê²€ìƒ‰
                        if st.session_state.retriever:
                            docs = st.session_state.retriever.search(prompt, k=search_k)

                            if docs:
                                # ì»¨í…ìŠ¤íŠ¸ ìƒì„±
                                context = "\n\n".join([doc.page_content[:500] for doc in docs])

                                # ì†ŒìŠ¤ ì •ë³´
                                sources = []
                                for doc in docs[:3]:
                                    title = doc.metadata.get('title', 'Unknown')
                                    category = doc.metadata.get('category', 'Unknown')
                                    sources.append(f"- {title} [{category}]")

                                # í”„ë¡¬í”„íŠ¸ ìƒì„±
                                full_prompt = f"""ë‹¤ìŒ ì»¨í…ìŠ¤íŠ¸ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— í•œêµ­ì–´ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”.

ì»¨í…ìŠ¤íŠ¸:
{context}

ì§ˆë¬¸: {prompt}

ë‹µë³€:"""

                                # LLM í˜¸ì¶œ
                                response = st.session_state.llm.invoke(full_prompt)
                                answer = response.content if hasattr(response, 'content') else str(response)

                                # ë‹µë³€ í‘œì‹œ
                                st.markdown(answer)

                                # ì†ŒìŠ¤ ì •ë³´ í‘œì‹œ
                                if sources:
                                    with st.expander("ğŸ“š ì°¸ì¡° ë¬¸ì„œ"):
                                        st.markdown("\n".join(sources))
                            else:
                                answer = "ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ì§ˆë¬¸ì„ í•´ì£¼ì„¸ìš”."
                                st.markdown(answer)
                        else:
                            # ê²€ìƒ‰ ì‹œìŠ¤í…œ ì—†ì´ LLMë§Œ ì‚¬ìš©
                            response = st.session_state.llm.invoke(prompt)
                            answer = response.content if hasattr(response, 'content') else str(response)
                            st.markdown(answer)
                            st.info("ğŸ’¡ ê²€ìƒ‰ ì‹œìŠ¤í…œì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•„ ì¼ë°˜ ì‘ë‹µì„ ì œê³µí•©ë‹ˆë‹¤.")

                        # ì‘ë‹µ ì €ì¥
                        st.session_state.messages.append({"role": "assistant", "content": answer})

                    except Exception as e:
                        error_msg = f"âŒ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
                        st.error(error_msg)
                        st.session_state.messages.append({"role": "assistant", "content": error_msg})

with tab2:
    st.header("ğŸ“š ë¬¸ì„œ ê´€ë¦¬")

    col1, col2 = st.columns(2)

    with col1:
        st.subheader("ğŸ“¥ ë¬¸ì„œ ìˆ˜ì§‘")

        urls_input = st.text_area(
            "ìˆ˜ì§‘í•  URL (í•œ ì¤„ì— í•˜ë‚˜ì”©)",
            value="https://python.langchain.com/docs/introduction\nhttps://python.langchain.com/docs/concepts",
            height=100
        )

        if st.button("ğŸ”„ ë¬¸ì„œ ìˆ˜ì§‘ ì‹œì‘"):
            if urls_input:
                urls = [url.strip() for url in urls_input.split('\n') if url.strip()]

                with st.spinner(f"{len(urls)}ê°œ ë¬¸ì„œ ìˆ˜ì§‘ ì¤‘..."):
                    try:
                        from data_collector import LangChainDataCollector

                        collector = LangChainDataCollector()
                        documents = collector.collect_documents(urls=urls, max_pages=len(urls))

                        if documents:
                            # ì²­í¬ ë¶„í• 
                            chunked_docs = collector.chunk_documents(documents)

                            # ë²¡í„° DBì— ì €ì¥
                            if st.session_state.vector_db:
                                st.session_state.vector_db.add_documents(chunked_docs)
                                st.success(f"âœ… {len(documents)}ê°œ ë¬¸ì„œ, {len(chunked_docs)}ê°œ ì²­í¬ ì €ì¥ ì™„ë£Œ!")
                            else:
                                st.warning("âš ï¸ ë²¡í„° DBê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                        else:
                            st.error("âŒ ë¬¸ì„œ ìˆ˜ì§‘ ì‹¤íŒ¨")

                    except Exception as e:
                        st.error(f"âŒ ì˜¤ë¥˜: {str(e)}")

    with col2:
        st.subheader("ğŸ“Š ë°ì´í„°ë² ì´ìŠ¤ í†µê³„")

        if st.button("ğŸ“ˆ í†µê³„ ì¡°íšŒ"):
            try:
                from data_collector import LangChainDataCollector

                collector = LangChainDataCollector()
                stats = collector.get_statistics()

                st.metric("ì´ ë¬¸ì„œ ìˆ˜", stats.get('total_documents', 0))

                if stats.get('documents_by_category'):
                    st.markdown("**ì¹´í…Œê³ ë¦¬ë³„ ë¬¸ì„œ:**")
                    for category, count in stats['documents_by_category'].items():
                        st.info(f"- {category}: {count}ê°œ")

                if st.session_state.vector_db:
                    vdb_stats = st.session_state.vector_db.get_statistics()
                    st.metric("ë²¡í„° DB ë¬¸ì„œ ìˆ˜", vdb_stats.get('document_count', 0))

            except Exception as e:
                st.error(f"âŒ ì˜¤ë¥˜: {str(e)}")

with tab3:
    st.header("ğŸ“ˆ ì‹œìŠ¤í…œ í†µê³„")

    col1, col2, col3 = st.columns(3)

    with col1:
        st.metric("ì´ ëŒ€í™” ìˆ˜", len(st.session_state.messages) // 2)
        st.metric("í˜„ì¬ ì„¸ì…˜ ë©”ì‹œì§€", len(st.session_state.messages))

    with col2:
        st.metric("LLM ìƒíƒœ", "ì—°ê²°ë¨ âœ…" if st.session_state.llm else "ë¯¸ì—°ê²° âŒ")
        st.metric("Vector DB ìƒíƒœ", "ì—°ê²°ë¨ âœ…" if st.session_state.vector_db else "ë¯¸ì—°ê²° âŒ")

    with col3:
        st.metric("ê²€ìƒ‰ ì‹œìŠ¤í…œ", "ì¤€ë¹„ë¨ âœ…" if st.session_state.retriever else "ë¯¸ì¤€ë¹„ âŒ")
        st.metric("Temperature", temperature)

    # ìµœê·¼ ëŒ€í™”
    if st.session_state.messages:
        st.markdown("---")
        st.subheader("ğŸ“ ìµœê·¼ ëŒ€í™”")

        for i in range(len(st.session_state.messages)-1, max(-1, len(st.session_state.messages)-6), -1):
            msg = st.session_state.messages[i]
            if msg["role"] == "user":
                st.markdown(f"**ğŸ‘¤ ì‚¬ìš©ì:** {msg['content'][:100]}...")
            else:
                st.markdown(f"**ğŸ¤– ì±—ë´‡:** {msg['content'][:100]}...")

# í‘¸í„°
st.markdown("---")
st.markdown(
    """
    <div style='text-align: center; color: gray;'>
    LangChain RAG ì±—ë´‡ v1.0 | Powered by Upstage Solar & ChromaDB
    </div>
    """,
    unsafe_allow_html=True
)