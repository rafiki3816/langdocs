# ğŸ” í˜‘ì—… ë¯¸ì…˜ ì²´í¬ë¦¬ìŠ¤íŠ¸ ë° TODO

## ğŸ“‹ í”„ë¡œì íŠ¸ ê°œìš”
- **í˜‘ì—… ëª©ì **: LangChain ë¬¸ì„œ ê¸°ë°˜ RAG ì‹œìŠ¤í…œ êµ¬ì¶•
- **ë‹´ë‹¹ ë¯¸ì…˜**:
  1. LangChain ë¬¸ì„œ í¬ë¡¤ë§ + ìµœì  ì²­í‚¹ ì „ëµ êµ¬í˜„
  2. Upstage ì„ë² ë”© ëª¨ë¸ ì„¤ì • + Vector DB ì ì¬ ìŠ¤í¬ë¦½íŠ¸ êµ¬í˜„

---

## âœ… ë¯¸ì…˜ 1: LangChain ë¬¸ì„œ í¬ë¡¤ë§ ë° ì²­í‚¹ ì „ëµ

### ğŸ“Œ í•µì‹¬ ìš”êµ¬ì‚¬í•­
- [x] LangChain ê³µì‹ ë¬¸ì„œ í¬ë¡¤ë§ ì½”ë“œ êµ¬í˜„
- [x] ì½”ë“œ ë¸”ë¡(````) ë³´ì¡´í•˜ëŠ” ì²­í‚¹ ì „ëµ ì ìš©
- [x] í•¨ìˆ˜ ì‹œê·¸ë‹ˆì²˜ ë³´ì¡´
- [x] **ë¬¸ì„œ ì²­í¬ ë¦¬ìŠ¤íŠ¸ë¥¼ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜** ì™„ì„±

### ğŸ” ì²´í¬ë¦¬ìŠ¤íŠ¸

#### 1.1 í¬ë¡¤ë§ ì½”ë“œ ê²€ì¦
- [x] **íŒŒì¼ ì¡´ì¬**: `data_collector.py` êµ¬í˜„ ì™„ë£Œ (527ì¤„)
- [x] **í¬ë¡¤ë§ ëŒ€ìƒ**: LangChain ê³µì‹ ë¬¸ì„œ (https://python.langchain.com)
- [x] **í¬ë¡¤ë§ ë²”ìœ„**: 63ê°œ ë¬¸ì„œ ìˆ˜ì§‘ ì™„ë£Œ
- [x] **ë©”íƒ€ë°ì´í„° í¬í•¨**: URL, ì œëª©, ì¹´í…Œê³ ë¦¬, íƒ€ì„ìŠ¤íƒ¬í”„ ì €ì¥
- [x] **ì—ëŸ¬ í•¸ë“¤ë§**: ì¬ì‹œë„ ë¡œì§, íƒ€ì„ì•„ì›ƒ ì„¤ì •

**ê²€ì¦ ëª…ë ¹ì–´**:
```bash
# í¬ë¡¤ë§ëœ ë¬¸ì„œ ìˆ˜ í™•ì¸
sqlite3 ./data/langchain.db "SELECT COUNT(*) FROM documents;"
# ê²°ê³¼: 63ê°œ ë¬¸ì„œ

# ì¹´í…Œê³ ë¦¬ë³„ ë¬¸ì„œ ë¶„í¬ í™•ì¸
sqlite3 ./data/langchain.db "SELECT category, COUNT(*) FROM documents GROUP BY category;"
```

#### 1.2 ì²­í‚¹ ì „ëµ êµ¬í˜„ ê²€ì¦
- [x] **êµ¬ì¡° ê¸°ë°˜ í…ìŠ¤íŠ¸ ë¶„í• ê¸°**: `advanced_text_splitter.py` (703ì¤„)
- [x] **ì½”ë“œ ë¸”ë¡ ë³´ì¡´**: ````ë¡œ ê°ì‹¸ì§„ ì½”ë“œ ë¸”ë¡ì„ í•˜ë‚˜ì˜ ì²­í¬ë¡œ ìœ ì§€
- [x] **í•¨ìˆ˜/í´ë˜ìŠ¤ ì •ì˜ ë³´ì¡´**: Python AST íŒŒì‹±ìœ¼ë¡œ í•¨ìˆ˜ ì „ì²´ ìœ ì§€
- [x] **Markdown êµ¬ì¡° ì¸ì‹**: í—¤ë”(#, ##, ###) ê¸°ë°˜ ë…¼ë¦¬ì  ë¶„í• 
- [x] **ì²­í¬ í¬ê¸° ìµœì í™”**:
  - ê¸°ë³¸ ì²­í¬: 1500ì
  - ì½”ë“œ ë¸”ë¡: ìµœëŒ€ 3000ì
  - ì˜¤ë²„ë©: 200ì

**ê²€ì¦ ëª…ë ¹ì–´**:
```bash
# ì²­í‚¹ ì „ëµ í…ŒìŠ¤íŠ¸
python -c "
from advanced_text_splitter import StructuredTextSplitter
splitter = StructuredTextSplitter(
    chunk_size=1500,
    chunk_overlap=200,
    preserve_code_blocks=True,
    preserve_functions=True
)
print('âœ“ êµ¬ì¡° ê¸°ë°˜ ë¶„í• ê¸° ì´ˆê¸°í™” ì„±ê³µ')
print(f'ì²­í¬ í¬ê¸°: {splitter.chunk_size}')
print(f'ì½”ë“œ ë¸”ë¡ ë³´ì¡´: {splitter.preserve_code_blocks}')
print(f'í•¨ìˆ˜ ë³´ì¡´: {splitter.preserve_functions}')
"
```

#### 1.3 ë¬¸ì„œ ì²­í¬ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜ í•¨ìˆ˜ ê²€ì¦
- [x] **í•µì‹¬ í•¨ìˆ˜**: `DataCollector.chunk_documents()`
- [x] **ì…ë ¥**: `List[Document]` (í¬ë¡¤ë§ëœ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸)
- [x] **ì¶œë ¥**: `List[Document]` (ì²­í¬ëœ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸)
- [x] **í•¨ìˆ˜ ì‹œê·¸ë‹ˆì²˜**:
```python
def chunk_documents(
    self,
    documents: List[Document],
    chunk_size: int = 1000,
    chunk_overlap: int = 200,
    use_structured_splitter: bool = True
) -> List[Document]:
```

**ê²€ì¦ ì½”ë“œ**:
```python
# data_collector.pyì—ì„œ í•¨ìˆ˜ ì‚¬ìš© ì˜ˆì œ
from data_collector import DataCollector

collector = DataCollector()

# 1. ë¬¸ì„œ ìˆ˜ì§‘
documents = collector.collect_documents(max_pages=5)
print(f"âœ“ ìˆ˜ì§‘ëœ ë¬¸ì„œ: {len(documents)}ê°œ")

# 2. êµ¬ì¡° ê¸°ë°˜ ì²­í‚¹ (ì½”ë“œ ë¸”ë¡ ë³´ì¡´)
chunked_docs = collector.chunk_documents(
    documents,
    chunk_size=1500,
    chunk_overlap=200,
    use_structured_splitter=True  # í•µì‹¬: êµ¬ì¡° ê¸°ë°˜ ë¶„í•  ì‚¬ìš©
)
print(f"âœ“ ì²­í¬ ê²°ê³¼: {len(chunked_docs)}ê°œ")

# 3. ë°˜í™˜ê°’ ê²€ì¦
assert isinstance(chunked_docs, list), "ë°˜í™˜ê°’ì´ ë¦¬ìŠ¤íŠ¸ê°€ ì•„ë‹˜"
assert all(hasattr(doc, 'page_content') for doc in chunked_docs), "Document ê°ì²´ ì•„ë‹˜"
print("âœ“ ë°˜í™˜ê°’ ê²€ì¦ ì„±ê³µ")
```

#### 1.4 ì²­í‚¹ í’ˆì§ˆ ê²€ì¦
- [x] **ì½”ë“œ ë¸”ë¡ ì™„ì „ì„±**: ``` ì‹œì‘ê³¼ ëì´ ê°™ì€ ì²­í¬ì— í¬í•¨
- [x] **í•¨ìˆ˜ ë¬´ê²°ì„±**: def/class ì •ì˜ê°€ ë¶„í• ë˜ì§€ ì•ŠìŒ
- [x] **ì»¨í…ìŠ¤íŠ¸ ìœ ì§€**: chunk_overlapìœ¼ë¡œ ë¬¸ë§¥ ì—°ê²°
- [x] **ë©”íƒ€ë°ì´í„° ì „ë‹¬**: ì›ë³¸ ë¬¸ì„œì˜ ë©”íƒ€ë°ì´í„°ê°€ ì²­í¬ì— ë³´ì¡´

**í’ˆì§ˆ í™•ì¸ ìŠ¤í¬ë¦½íŠ¸**:
```bash
# ì²­í‚¹ í’ˆì§ˆ ê²€ì¦
python -c "
from data_collector import DataCollector
import re

collector = DataCollector()
docs = collector.collect_documents(max_pages=3)
chunks = collector.chunk_documents(docs, use_structured_splitter=True)

# ì½”ë“œ ë¸”ë¡ ì™„ì „ì„± ê²€ì‚¬
broken_blocks = 0
for chunk in chunks:
    content = chunk.page_content
    open_count = content.count('\`\`\`')
    if open_count % 2 != 0:
        broken_blocks += 1

print(f'âœ“ ì´ ì²­í¬: {len(chunks)}ê°œ')
print(f'âœ“ ê¹¨ì§„ ì½”ë“œ ë¸”ë¡: {broken_blocks}ê°œ')
print(f'âœ“ ì²­í‚¹ í’ˆì§ˆ: {\"í†µê³¼\" if broken_blocks == 0 else \"í™•ì¸ í•„ìš”\"}')
"
```

### ğŸ“„ ë¯¸ì…˜ 1 ê´€ë ¨ íŒŒì¼ ëª©ë¡
```
âœ“ data_collector.py          # í¬ë¡¤ëŸ¬ + ì²­í‚¹ í•¨ìˆ˜ (í•µì‹¬)
âœ“ advanced_text_splitter.py  # êµ¬ì¡° ê¸°ë°˜ ë¶„í• ê¸° (í•µì‹¬)
âœ“ utils.py                   # ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜
âœ“ README.md                  # ì²­í‚¹ ì „ëµ ë¬¸ì„œí™”
âœ“ data/langchain.db          # í¬ë¡¤ë§ ê²°ê³¼ (63ê°œ ë¬¸ì„œ)
```

---

## âœ… ë¯¸ì…˜ 2: Upstage ì„ë² ë”© ëª¨ë¸ + Vector DB ì ì¬

### ğŸ“Œ í•µì‹¬ ìš”êµ¬ì‚¬í•­
- [x] Upstage ì„ë² ë”© ëª¨ë¸ ì„¤ì •
- [x] íŒ€ì› 4ì˜ ì²­í¬ ë°ì´í„°ë¥¼ ë°›ì•„ ì²˜ë¦¬
- [x] chroma_vector_db ì»¨í…Œì´ë„ˆì— ì ì¬í•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸
- [x] ìµœì´ˆ ë°ì´í„° ë¡œë”© ê¸°ëŠ¥

### ğŸ” ì²´í¬ë¦¬ìŠ¤íŠ¸

#### 2.1 Upstage ì„ë² ë”© ëª¨ë¸ ì„¤ì • ê²€ì¦
- [x] **API í‚¤ ì„¤ì •**: `.env` íŒŒì¼ì— `UPSTAGE_API_KEY` ì €ì¥
- [x] **ëª¨ë¸ ì„ íƒ**: `solar-embedding-1-large` (4096 ì°¨ì›)
- [x] **LangChain í†µí•©**: `UpstageEmbeddings` í´ë˜ìŠ¤ ì‚¬ìš©
- [x] **ì—ëŸ¬ í•¸ë“¤ë§**: API í‚¤ ê²€ì¦ ë¡œì§ í¬í•¨

**ê²€ì¦ ëª…ë ¹ì–´**:
```bash
# API í‚¤ í™•ì¸
cat .env | grep UPSTAGE_API_KEY

# ì„ë² ë”© ëª¨ë¸ í…ŒìŠ¤íŠ¸
python -c "
from llm import get_embeddings

embeddings = get_embeddings(model='solar-embedding-1-large')
test_vector = embeddings.embed_query('í…ŒìŠ¤íŠ¸ ë¬¸ì¥')
print(f'âœ“ ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì„±ê³µ')
print(f'âœ“ ë²¡í„° ì°¨ì›: {len(test_vector)}ì°¨ì›')
print(f'âœ“ ëª¨ë¸ëª…: solar-embedding-1-large')
"
```

**ì„¤ì • íŒŒì¼ í™•ì¸**:
```python
# llm.py:53-82
def get_embeddings(
    model: str = "solar-embedding-1-large"
) -> Embeddings:
    """
    Upstage Solar Embeddings ì¸ìŠ¤í„´ìŠ¤ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.

    Args:
        model: ì‚¬ìš©í•  ì„ë² ë”© ëª¨ë¸ëª…
            - "solar-embedding-1-large": ì¼ë°˜ìš©
            - "solar-embedding-1-large-query": ì¿¼ë¦¬ ìµœì í™”
            - "solar-embedding-1-large-passage": ë¬¸ì„œ ìµœì í™”
    """
    api_key = os.getenv("UPSTAGE_API_KEY")

    if not api_key or api_key == "your-actual-api-key-here":
        raise ValueError("UPSTAGE_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

    return UpstageEmbeddings(
        api_key=api_key,
        model=model
    )
```

#### 2.2 ChromaDB ì»¨í…Œì´ë„ˆ í™˜ê²½ ê²€ì¦
- [x] **Docker ì„¤ì •**: `docker-compose.yml` êµ¬í˜„
- [x] **ChromaDB ì´ë¯¸ì§€**: `chromadb/chroma:latest`
- [x] **í¬íŠ¸ ë§¤í•‘**: 8000:8000
- [x] **ì˜êµ¬ ì €ì¥ì†Œ**: `./data/chroma_docker` ë³¼ë¥¨ ë§ˆìš´íŠ¸
- [x] **í—¬ìŠ¤ì²´í¬**: API ì—”ë“œí¬ì¸íŠ¸ í™•ì¸

**Docker í™˜ê²½ ê²€ì¦**:
```bash
# Docker Compose íŒŒì¼ í™•ì¸
cat docker-compose.yml

# ChromaDB ì»¨í…Œì´ë„ˆ ì‹œì‘
./docker_run.sh start

# ì»¨í…Œì´ë„ˆ ìƒíƒœ í™•ì¸
docker ps | grep chroma

# ChromaDB í—¬ìŠ¤ì²´í¬
curl http://localhost:8000/api/v1/heartbeat
# ì˜ˆìƒ ì‘ë‹µ: {"nanosecond heartbeat": ...}

# ì»¨í…Œì´ë„ˆ ë¡œê·¸ í™•ì¸
./docker_run.sh logs chromadb
```

#### 2.3 Vector DB ì ì¬ ìŠ¤í¬ë¦½íŠ¸ ê²€ì¦
- [x] **í•µì‹¬ ìŠ¤í¬ë¦½íŠ¸**: `initialize_vector_db.py` (333ì¤„)
- [x] **ê¸°ëŠ¥**:
  - íŒ€ì›ì˜ ì²­í¬ ë°ì´í„° ì…ë ¥ ë°›ê¸°
  - ChromaDB ì»¨í…Œì´ë„ˆ ì—°ê²°
  - ì„ë² ë”© ìƒì„± ë° ì €ì¥
  - ë°°ì¹˜ ì²˜ë¦¬ (100ê°œì”©)
  - ì§„í–‰ë¥  í‘œì‹œ

**ìŠ¤í¬ë¦½íŠ¸ êµ¬ì¡° í™•ì¸**:
```python
# initialize_vector_db.pyì˜ í•µì‹¬ í´ë˜ìŠ¤
class VectorDBInitializer:
    def __init__(self, use_docker: bool = False):
        """
        Args:
            use_docker: Trueì´ë©´ Docker ChromaDB ì‚¬ìš©
        """
        self.use_docker = use_docker
        if use_docker:
            from vector_database_docker import DockerVectorDatabase
            self.vector_db = DockerVectorDatabase()
        else:
            from vector_database import VectorDatabase
            self.vector_db = VectorDatabase()

    def load_to_vector_db(
        self,
        documents: List[Document],
        batch_size: int = 100,
        show_progress: bool = True
    ) -> int:
        """íŒ€ì› ì²­í¬ ë°ì´í„°ë¥¼ Vector DBì— ì ì¬"""
        # ë°°ì¹˜ ì²˜ë¦¬ ë° ì§„í–‰ë¥  í‘œì‹œ
```

#### 2.4 íŒ€ì› ë°ì´í„° ìˆ˜ì‹  ì¸í„°í˜ì´ìŠ¤
- [x] **ì…ë ¥ í˜•ì‹**: `List[Document]` (LangChain Document ê°ì²´)
- [x] **í•„ìˆ˜ í•„ë“œ**:
  - `page_content`: ì²­í¬ í…ìŠ¤íŠ¸ (str)
  - `metadata`: ë©”íƒ€ë°ì´í„° ë”•ì…”ë„ˆë¦¬
    - `source`: ì¶œì²˜ URL
    - `title`: ë¬¸ì„œ ì œëª©
    - `chunk_index`: ì²­í¬ ìˆœì„œ

**íŒ€ì› 4ë¡œë¶€í„° ë°ì´í„° ë°›ëŠ” ì˜ˆì œ ì½”ë“œ**:
```python
# íŒ€ì› 4ê°€ ì œê³µí•  ì²­í¬ ë°ì´í„° í˜•ì‹
from langchain.schema import Document

# ì˜ˆì‹œ: íŒ€ì›ì´ ì´ í˜•ì‹ìœ¼ë¡œ ì²­í¬ ë¦¬ìŠ¤íŠ¸ë¥¼ ì „ë‹¬
team_member_chunks = [
    Document(
        page_content="LangChainì€ LLM ì• í”Œë¦¬ì¼€ì´ì…˜ ê°œë°œ í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤...",
        metadata={
            "source": "https://python.langchain.com/docs/get_started",
            "title": "LangChain ì†Œê°œ",
            "chunk_index": 0
        }
    ),
    # ... ë” ë§ì€ ì²­í¬
]

# ìš°ë¦¬ì˜ ìŠ¤í¬ë¦½íŠ¸ë¡œ ì ì¬
from initialize_vector_db import VectorDBInitializer

initializer = VectorDBInitializer(use_docker=True)
loaded_count = initializer.load_to_vector_db(
    documents=team_member_chunks,  # íŒ€ì›ì´ ì œê³µí•œ ì²­í¬
    batch_size=100,
    show_progress=True
)
print(f"âœ“ {loaded_count}ê°œ ì²­í¬ ì ì¬ ì™„ë£Œ")
```

#### 2.5 ìµœì´ˆ ë°ì´í„° ë¡œë”© ê²€ì¦
- [x] **ì „ì²´ íŒŒì´í”„ë¼ì¸**: `run_full_pipeline()` ë©”ì„œë“œ
- [x] **ë‹¨ê³„**:
  1. ë¬¸ì„œ ìˆ˜ì§‘ (í¬ë¡¤ë§)
  2. êµ¬ì¡° ê¸°ë°˜ ì²­í‚¹
  3. Vector DB ì ì¬
- [x] **ì˜µì…˜**:
  - `--docker`: Docker ChromaDB ì‚¬ìš©
  - `--reset`: ê¸°ì¡´ ë°ì´í„° ì‚­ì œ í›„ ì¬ë¡œë”©
  - `--max-pages`: ë¡œë”©í•  ë¬¸ì„œ ìˆ˜ ì œí•œ
  - `--test-only`: í…ŒìŠ¤íŠ¸ ëª¨ë“œ (5ê°œ ë¬¸ì„œë§Œ)

**ìµœì´ˆ ë¡œë”© ì‹¤í–‰ ëª…ë ¹ì–´**:
```bash
# ë°©ë²• 1: ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ (Docker)
python initialize_vector_db.py --docker --reset --max-pages 30

# ë°©ë²• 2: Docker ìŠ¤í¬ë¦½íŠ¸ ì‚¬ìš©
./docker_run.sh init-db

# ë°©ë²• 3: í…ŒìŠ¤íŠ¸ ëª¨ë“œ (5ê°œ ë¬¸ì„œë§Œ)
python initialize_vector_db.py --docker --test-only

# ë¡œë”© ê²°ê³¼ í™•ì¸
python -c "
from vector_database_docker import DockerVectorDatabase
vdb = DockerVectorDatabase()
stats = vdb.get_statistics()
print(f'âœ“ ì ì¬ëœ ë¬¸ì„œ: {stats[\"total_documents\"]}ê°œ')
print(f'âœ“ ì»¬ë ‰ì…˜: {stats[\"collection_name\"]}')
"
```

#### 2.6 ì ì¬ í’ˆì§ˆ ê²€ì¦
- [x] **ë°ì´í„° ë¬´ê²°ì„±**: ëª¨ë“  ì²­í¬ê°€ ì €ì¥ë¨
- [x] **ì„ë² ë”© ì°¨ì›**: 4096ì°¨ì› ë²¡í„° í™•ì¸
- [x] **ë©”íƒ€ë°ì´í„° ë³´ì¡´**: source, title ë“± ìœ ì§€
- [x] **ê²€ìƒ‰ í…ŒìŠ¤íŠ¸**: ìƒ˜í”Œ ì¿¼ë¦¬ë¡œ ê²€ìƒ‰ ë™ì‘ í™•ì¸

**í’ˆì§ˆ ê²€ì¦ ìŠ¤í¬ë¦½íŠ¸**:
```bash
# Vector DB í’ˆì§ˆ ê²€ì¦
python -c "
from vector_database_docker import DockerVectorDatabase
from llm import get_embeddings

# 1. Vector DB ì—°ê²°
vdb = DockerVectorDatabase()
vdb.init_vectorstore()

# 2. í†µê³„ í™•ì¸
stats = vdb.get_statistics()
print('=== Vector DB í†µê³„ ===')
print(f'ì´ ë¬¸ì„œ: {stats[\"total_documents\"]}ê°œ')
print(f'ì»¬ë ‰ì…˜: {stats[\"collection_name\"]}')

# 3. ìƒ˜í”Œ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
results = vdb.search('LangChainì´ë€ ë¬´ì—‡ì¸ê°€?', k=3)
print(f'\nâœ“ ê²€ìƒ‰ ê²°ê³¼: {len(results)}ê°œ')
print(f'âœ“ ì²« ë²ˆì§¸ ê²°ê³¼: {results[0].page_content[:100]}...')

# 4. ì„ë² ë”© ì°¨ì› í™•ì¸
embeddings = get_embeddings()
test_vec = embeddings.embed_query('í…ŒìŠ¤íŠ¸')
print(f'\nâœ“ ì„ë² ë”© ì°¨ì›: {len(test_vec)}ì°¨ì›')
"
```

### ğŸ“„ ë¯¸ì…˜ 2 ê´€ë ¨ íŒŒì¼ ëª©ë¡
```
âœ“ llm.py                        # Upstage ì„ë² ë”© ëª¨ë¸ ì„¤ì • (í•µì‹¬)
âœ“ vector_database_docker.py     # Docker ChromaDB í´ë¼ì´ì–¸íŠ¸ (í•µì‹¬)
âœ“ initialize_vector_db.py       # ì ì¬ ìŠ¤í¬ë¦½íŠ¸ (í•µì‹¬)
âœ“ docker-compose.yml            # ì»¨í…Œì´ë„ˆ ì„¤ì •
âœ“ Dockerfile                    # ì•± ì´ë¯¸ì§€ ë¹Œë“œ
âœ“ docker_run.sh                 # Docker ê´€ë¦¬ ìŠ¤í¬ë¦½íŠ¸
âœ“ .env                          # API í‚¤ ì €ì¥
âœ“ data/chroma_docker/           # Vector DB ë°ì´í„° ì €ì¥ì†Œ
```

---

## ğŸ“Š í†µí•© ê²€ì¦ ì ˆì°¨

### Step 1: í™˜ê²½ ì¤€ë¹„
```bash
# 1. ê°€ìƒí™˜ê²½ í™œì„±í™”
source venv/bin/activate

# 2. ì˜ì¡´ì„± í™•ì¸
pip list | grep -E "langchain|chromadb|upstage"

# 3. API í‚¤ í™•ì¸
cat .env | grep UPSTAGE_API_KEY

# 4. Docker í™•ì¸
docker --version
docker-compose --version
```

### Step 2: ë¯¸ì…˜ 1 ê²€ì¦ (í¬ë¡¤ë§ + ì²­í‚¹)
```bash
# 1. ì²­í‚¹ ëª¨ë“ˆ í…ŒìŠ¤íŠ¸
python advanced_text_splitter.py

# 2. í¬ë¡¤ë§ + ì²­í‚¹ í†µí•© í…ŒìŠ¤íŠ¸
python -c "
from data_collector import DataCollector

collector = DataCollector()

# ì†Œê·œëª¨ í…ŒìŠ¤íŠ¸ (5ê°œ ë¬¸ì„œ)
docs = collector.collect_documents(max_pages=5)
print(f'âœ“ í¬ë¡¤ë§: {len(docs)}ê°œ')

# êµ¬ì¡° ê¸°ë°˜ ì²­í‚¹
chunks = collector.chunk_documents(docs, use_structured_splitter=True)
print(f'âœ“ ì²­í‚¹: {len(chunks)}ê°œ')

# ë°˜í™˜ê°’ íƒ€ì… í™•ì¸
print(f'âœ“ ë°˜í™˜ íƒ€ì…: {type(chunks)}')
print(f'âœ“ ì²« ì²­í¬ íƒ€ì…: {type(chunks[0])}')
"

# 3. ì½”ë“œ ë¸”ë¡ ë³´ì¡´ ê²€ì¦
python -c "
from data_collector import DataCollector
collector = DataCollector()
docs = collector.collect_documents(max_pages=3)
chunks = collector.chunk_documents(docs, use_structured_splitter=True)

# ì½”ë“œ ë¸”ë¡ ì™„ì „ì„± ì²´í¬
broken = sum(1 for c in chunks if c.page_content.count('\`\`\`') % 2 != 0)
print(f'âœ“ ì´ ì²­í¬: {len(chunks)}ê°œ')
print(f'âœ“ ê¹¨ì§„ ì½”ë“œ ë¸”ë¡: {broken}ê°œ')
print(f'âœ“ ê²°ê³¼: {\"í†µê³¼\" if broken == 0 else \"í™•ì¸ í•„ìš”\"}')
"
```

### Step 3: ë¯¸ì…˜ 2 ê²€ì¦ (ì„ë² ë”© + Vector DB)
```bash
# 1. ì„ë² ë”© ëª¨ë¸ í…ŒìŠ¤íŠ¸
python llm.py

# 2. Docker ChromaDB ì‹œì‘
./docker_run.sh start

# 3. ChromaDB ì—°ê²° í…ŒìŠ¤íŠ¸
curl http://localhost:8000/api/v1/heartbeat

# 4. í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë”©
python initialize_vector_db.py --docker --test-only

# 5. Vector DB ê²€ì¦
python -c "
from vector_database_docker import DockerVectorDatabase
vdb = DockerVectorDatabase()
vdb.init_vectorstore()
stats = vdb.get_statistics()
print(f'âœ“ Vector DB ë¬¸ì„œ: {stats[\"total_documents\"]}ê°œ')

# ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
results = vdb.search('LangChain ë©”ëª¨ë¦¬', k=3)
print(f'âœ“ ê²€ìƒ‰ ê¸°ëŠ¥: {\"ì •ìƒ\" if len(results) > 0 else \"ì˜¤ë¥˜\"}')
"
```

### Step 4: ì „ì²´ íŒŒì´í”„ë¼ì¸ ê²€ì¦
```bash
# ì „ì²´ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ (30ê°œ ë¬¸ì„œ)
python initialize_vector_db.py --docker --reset --max-pages 30

# ê²°ê³¼ í™•ì¸
python -c "
from vector_database_docker import DockerVectorDatabase
vdb = DockerVectorDatabase()
vdb.init_vectorstore()
stats = vdb.get_statistics()
print('=== ìµœì¢… ê²€ì¦ ê²°ê³¼ ===')
print(f'âœ“ ì ì¬ ì™„ë£Œ: {stats[\"total_documents\"]}ê°œ ë¬¸ì„œ')
print(f'âœ“ Vector DB: {stats[\"collection_name\"]}')

# SQLite DB í™•ì¸
import sqlite3
conn = sqlite3.connect('./data/langchain.db')
cursor = conn.cursor()
cursor.execute('SELECT COUNT(*) FROM documents')
doc_count = cursor.fetchone()[0]
conn.close()
print(f'âœ“ SQLite DB: {doc_count}ê°œ ë¬¸ì„œ')
"
```

---

## ğŸ“ TODO ë¦¬ìŠ¤íŠ¸ (GitHub ì—…ë°ì´íŠ¸ ì „)

### ğŸ”´ í•„ìˆ˜ ì‚¬í•­ (Must Have)
- [ ] **1. ë¯¸ì…˜ 1 ê²€ì¦ ì™„ë£Œ**
  - [ ] `advanced_text_splitter.py` ì‹¤í–‰ í…ŒìŠ¤íŠ¸ í†µê³¼
  - [ ] `DataCollector.chunk_documents()` í•¨ìˆ˜ ë°˜í™˜ê°’ ê²€ì¦
  - [ ] ì½”ë“œ ë¸”ë¡ ë³´ì¡´ í™•ì¸ (ê¹¨ì§„ ë¸”ë¡ 0ê°œ)
  - [ ] README.mdì— ì²­í‚¹ ì „ëµ ë¬¸ì„œí™” í™•ì¸

- [ ] **2. ë¯¸ì…˜ 2 ê²€ì¦ ì™„ë£Œ**
  - [ ] `.env` íŒŒì¼ì— `UPSTAGE_API_KEY` ì„¤ì •
  - [ ] `llm.py` ì‹¤í–‰í•˜ì—¬ ì„ë² ë”© ëª¨ë¸ í…ŒìŠ¤íŠ¸ í†µê³¼
  - [ ] Docker ChromaDB ì»¨í…Œì´ë„ˆ ì •ìƒ ì‹¤í–‰ í™•ì¸
  - [ ] `initialize_vector_db.py --docker --test-only` ì„±ê³µ

- [ ] **3. í†µí•© í…ŒìŠ¤íŠ¸**
  - [ ] ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ (30ê°œ ë¬¸ì„œ)
  - [ ] Vector DB ê²€ìƒ‰ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸
  - [ ] íŒ€ì› ë°ì´í„° ìˆ˜ì‹  ì¸í„°í˜ì´ìŠ¤ ë¬¸ì„œí™”

- [ ] **4. ë¬¸ì„œí™”**
  - [ ] README.md ìµœì‹ í™”
  - [ ] ì´ ì²´í¬ë¦¬ìŠ¤íŠ¸ íŒŒì¼ (`COLLABORATION_CHECKLIST.md`)
  - [ ] `EXECUTION_GUIDE.md` í™•ì¸
  - [ ] íŒ€ì›ì„ ìœ„í•œ ì‚¬ìš©ë²• ì„¹ì…˜ ì¶”ê°€

- [ ] **5. Git ì¤€ë¹„**
  - [ ] ë¶ˆí•„ìš”í•œ íŒŒì¼ `.gitignore`ì— ì¶”ê°€
  - [ ] ì»¤ë°‹ ë©”ì‹œì§€ ì¤€ë¹„
  - [ ] ì›ê²© ì €ì¥ì†Œ ë™ê¸°í™” í™•ì¸

### ğŸŸ¡ ê¶Œì¥ ì‚¬í•­ (Nice to Have)
- [ ] ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ (í¬ë¡¤ë§/ì²­í‚¹/ì„ë² ë”© ì†ë„)
- [ ] ì—ëŸ¬ ì¼€ì´ìŠ¤ í…ŒìŠ¤íŠ¸ (ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜, API ì œí•œ ë“±)
- [ ] ë¡œê¹… ë ˆë²¨ í™•ì¸ ë° ì •ë¦¬
- [ ] Docker ì´ë¯¸ì§€ ìµœì í™”

### ğŸŸ¢ ì„ íƒ ì‚¬í•­ (Optional)
- [ ] CI/CD íŒŒì´í”„ë¼ì¸ ì„¤ì •
- [ ] ë‹¨ìœ„ í…ŒìŠ¤íŠ¸ ì¶”ê°€
- [ ] ì½”ë“œ ìŠ¤íƒ€ì¼ ê²€ì‚¬ (flake8, black)

---

## ğŸš€ GitHub ì—…ë°ì´íŠ¸ ì ˆì°¨

### 1. ìµœì¢… ê²€ì¦
```bash
# ì „ì²´ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸
./docker_run.sh start
python initialize_vector_db.py --docker --test-only

# ê²°ê³¼ í™•ì¸
./docker_run.sh status
```

### 2. Git ì»¤ë°‹
```bash
# 1. ë³€ê²½ì‚¬í•­ í™•ì¸
git status

# 2. ì¶”ê°€í•  íŒŒì¼ í™•ì¸
git add .

# 3. ì»¤ë°‹ (ì˜ë¯¸ ìˆëŠ” ë©”ì‹œì§€)
git commit -m "feat: LangChain í¬ë¡¤ë§ + Upstage ì„ë² ë”© í†µí•© ì™„ë£Œ

- ë¯¸ì…˜ 1: êµ¬ì¡° ê¸°ë°˜ ì²­í‚¹ ì „ëµ êµ¬í˜„ (ì½”ë“œ ë¸”ë¡ ë³´ì¡´)
  - advanced_text_splitter.py: ì½”ë“œ/í•¨ìˆ˜ ë³´ì¡´ ë¡œì§
  - data_collector.py: chunk_documents() í•¨ìˆ˜ ì™„ì„±

- ë¯¸ì…˜ 2: Upstage ì„ë² ë”© + ChromaDB ì»¨í…Œì´ë„ˆ ì„¤ì •
  - llm.py: solar-embedding-1-large ì„¤ì •
  - vector_database_docker.py: Docker í´ë¼ì´ì–¸íŠ¸
  - initialize_vector_db.py: íŒ€ì› ë°ì´í„° ì ì¬ ìŠ¤í¬ë¦½íŠ¸

- ë¬¸ì„œí™”: COLLABORATION_CHECKLIST.md ì¶”ê°€"
```

### 3. GitHub í‘¸ì‹œ
```bash
# ì›ê²© ì €ì¥ì†Œ í™•ì¸
git remote -v

# í‘¸ì‹œ
git push origin main

# í‘¸ì‹œ í›„ í™•ì¸
git log -1
```

### 4. íŒ€ì›ì—ê²Œ ê³µìœ í•  ì •ë³´
```markdown
## íŒ€ì› 4ë‹˜ê»˜

ì•ˆë…•í•˜ì„¸ìš”! LangChain ë¬¸ì„œ í¬ë¡¤ë§ ë° Vector DB ì ì¬ ì‹œìŠ¤í…œì´ ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤.

### ì²­í¬ ë°ì´í„° ì „ë‹¬ í˜•ì‹
ì²­í¬ ë°ì´í„°ë¥¼ ì•„ë˜ í˜•ì‹ìœ¼ë¡œ ì „ë‹¬í•´ì£¼ì„¸ìš”:

\`\`\`python
from langchain.schema import Document

chunks = [
    Document(
        page_content="ì²­í¬ í…ìŠ¤íŠ¸ ë‚´ìš©...",
        metadata={
            "source": "ë¬¸ì„œ URL",
            "title": "ë¬¸ì„œ ì œëª©",
            "chunk_index": 0
        }
    ),
    # ... ë” ë§ì€ ì²­í¬
]
\`\`\`

### ì ì¬ ë°©ë²•
\`\`\`python
from initialize_vector_db import VectorDBInitializer

# Docker ChromaDB ì‹œì‘
# ./docker_run.sh start

# ì²­í¬ ì ì¬
initializer = VectorDBInitializer(use_docker=True)
loaded_count = initializer.load_to_vector_db(
    documents=chunks,  # ì—¬ëŸ¬ë¶„ì´ ì œê³µí•œ ì²­í¬
    batch_size=100
)
print(f"ì ì¬ ì™„ë£Œ: {loaded_count}ê°œ")
\`\`\`

### ì°¸ê³  ë¬¸ì„œ
- ì‹¤í–‰ ê°€ì´ë“œ: `EXECUTION_GUIDE.md`
- í˜‘ì—… ì²´í¬ë¦¬ìŠ¤íŠ¸: `COLLABORATION_CHECKLIST.md`
- Docker ì‚¬ìš©ë²•: `./docker_run.sh help`
```

---

## ğŸ” íŠ¸ëŸ¬ë¸”ìŠˆíŒ…

### ë¬¸ì œ 1: "UPSTAGE_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤"
```bash
# .env íŒŒì¼ í™•ì¸
cat .env

# API í‚¤ ì¶”ê°€
echo "UPSTAGE_API_KEY=your-actual-key" >> .env
```

### ë¬¸ì œ 2: Docker ChromaDB ì—°ê²° ì‹¤íŒ¨
```bash
# ì»¨í…Œì´ë„ˆ ìƒíƒœ í™•ì¸
docker ps | grep chroma

# ì¬ì‹œì‘
./docker_run.sh restart

# ë¡œê·¸ í™•ì¸
./docker_run.sh logs chromadb
```

### ë¬¸ì œ 3: ì²­í‚¹ ì¤‘ ì½”ë“œ ë¸”ë¡ì´ ê¹¨ì§
```python
# advanced_text_splitter.py ì„¤ì • í™•ì¸
splitter = StructuredTextSplitter(
    preserve_code_blocks=True,  # ë°˜ë“œì‹œ True
    code_block_max_size=3000    # ì¶©ë¶„íˆ í° ê°’
)
```

---

## ï¿½ï¿½ ì—°ë½ì²˜ ë° ë¦¬ì†ŒìŠ¤

- **GitHub ì €ì¥ì†Œ**: https://github.com/rafiki3816/langdocs.git
- **LangChain ë¬¸ì„œ**: https://python.langchain.com
- **Upstage ì½˜ì†”**: https://console.upstage.ai
- **ChromaDB ë¬¸ì„œ**: https://docs.trychroma.com

---

**ì‘ì„±ì¼**: 2025-10-30
**ë²„ì „**: 1.0
**ìƒíƒœ**: âœ… ë¯¸ì…˜ 1, 2 êµ¬í˜„ ì™„ë£Œ | ğŸ”„ GitHub ì—…ë°ì´íŠ¸ ëŒ€ê¸°
