#!/usr/bin/env python3
"""
Vector DB ì´ˆê¸°í™” ìŠ¤í¬ë¦½íŠ¸
LangChain ë¬¸ì„œë¥¼ ìˆ˜ì§‘í•˜ê³  êµ¬ì¡° ê¸°ë°˜ ì²­í‚¹ í›„ Vector DBì— ì ì¬
"""

import os
import sys
import argparse
from typing import List, Optional, Dict, Any
from pathlib import Path
from datetime import datetime

# í”„ë¡œì íŠ¸ ëª¨ë“ˆ ì„í¬íŠ¸
from data_collector import LangChainDataCollector
from advanced_text_splitter import StructuredTextSplitter, create_smart_splitter
from vector_database import VectorDatabase
from vector_database_docker import DockerVectorDatabase, create_docker_vector_db
from langchain.schema import Document


# LangChain ë¬¸ì„œ URL ëª©ë¡ (í™•ì¥ ê°€ëŠ¥)
LANGCHAIN_URLS = [
    # í•µì‹¬ ê°œë…
    "https://python.langchain.com/docs/introduction",
    "https://python.langchain.com/docs/get_started/introduction",
    "https://python.langchain.com/docs/get_started/quickstart",
    "https://python.langchain.com/docs/get_started/installation",

    # ì£¼ìš” ëª¨ë“ˆ
    "https://python.langchain.com/docs/modules/model_io",
    "https://python.langchain.com/docs/modules/model_io/llms",
    "https://python.langchain.com/docs/modules/model_io/chat",
    "https://python.langchain.com/docs/modules/model_io/prompts",
    "https://python.langchain.com/docs/modules/model_io/output_parsers",

    # ë°ì´í„° ì—°ê²°
    "https://python.langchain.com/docs/modules/data_connection",
    "https://python.langchain.com/docs/modules/data_connection/document_loaders",
    "https://python.langchain.com/docs/modules/data_connection/document_transformers",
    "https://python.langchain.com/docs/modules/data_connection/text_embedding",
    "https://python.langchain.com/docs/modules/data_connection/vectorstores",
    "https://python.langchain.com/docs/modules/data_connection/retrievers",

    # ì²´ì¸
    "https://python.langchain.com/docs/modules/chains",
    "https://python.langchain.com/docs/modules/chains/foundational/llm_chain",
    "https://python.langchain.com/docs/modules/chains/foundational/sequential_chains",

    # ë©”ëª¨ë¦¬
    "https://python.langchain.com/docs/modules/memory",
    "https://python.langchain.com/docs/modules/memory/types/buffer",
    "https://python.langchain.com/docs/modules/memory/types/summary",

    # ì—ì´ì „íŠ¸
    "https://python.langchain.com/docs/modules/agents",
    "https://python.langchain.com/docs/modules/agents/agent_types",
    "https://python.langchain.com/docs/modules/agents/tools",

    # ì‚¬ìš© ì‚¬ë¡€
    "https://python.langchain.com/docs/use_cases/question_answering",
    "https://python.langchain.com/docs/use_cases/chatbots",
    "https://python.langchain.com/docs/use_cases/summarization",
]


class VectorDBInitializer:
    """Vector DB ì´ˆê¸°í™” ë° ë°ì´í„° ë¡œë”© í´ë˜ìŠ¤"""

    def __init__(
        self,
        use_docker: bool = False,
        docker_host: str = "localhost",
        docker_port: int = 8000,
        collection_name: str = "langchain_docs",
        embedding_model: str = "solar-embedding-1-large"
    ):
        """
        ì´ˆê¸°í™”

        Args:
            use_docker: Docker ChromaDB ì‚¬ìš© ì—¬ë¶€
            docker_host: Docker ChromaDB í˜¸ìŠ¤íŠ¸
            docker_port: Docker ChromaDB í¬íŠ¸
            collection_name: ì»¬ë ‰ì…˜ ì´ë¦„
            embedding_model: ì„ë² ë”© ëª¨ë¸
        """
        self.use_docker = use_docker
        self.collection_name = collection_name
        self.embedding_model = embedding_model

        # Vector DB ì„¤ì •
        if use_docker:
            print(f"ğŸ³ Docker ChromaDB ì‚¬ìš©: {docker_host}:{docker_port}")
            self.vector_db = DockerVectorDatabase(
                host=docker_host,
                port=docker_port,
                collection_name=collection_name,
                embedding_model=embedding_model
            )
        else:
            print("ğŸ’¾ ë¡œì»¬ ChromaDB ì‚¬ìš©")
            self.vector_db = VectorDatabase(
                persist_directory="./data/chroma_db",
                collection_name=collection_name
            )

        # ë°ì´í„° ìˆ˜ì§‘ê¸°
        self.collector = LangChainDataCollector()

        # êµ¬ì¡° ê¸°ë°˜ í…ìŠ¤íŠ¸ ë¶„í• ê¸°
        self.text_splitter = StructuredTextSplitter(
            chunk_size=1500,
            chunk_overlap=200,
            code_block_max_size=3000,
            preserve_code_blocks=True,
            preserve_functions=True,
            preserve_markdown_structure=True
        )

    def collect_documents(
        self,
        urls: List[str] = None,
        max_pages: int = 50
    ) -> List[Document]:
        """
        ë¬¸ì„œ ìˆ˜ì§‘

        Args:
            urls: ìˆ˜ì§‘í•  URL ëª©ë¡
            max_pages: ìµœëŒ€ ìˆ˜ì§‘ í˜ì´ì§€ ìˆ˜

        Returns:
            ìˆ˜ì§‘ëœ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸
        """
        print("\nğŸ“¥ ë¬¸ì„œ ìˆ˜ì§‘ ì‹œì‘...")

        if urls is None:
            urls = LANGCHAIN_URLS

        # ë¬¸ì„œ ìˆ˜ì§‘
        documents = self.collector.collect_documents(
            urls=urls[:max_pages],
            max_pages=max_pages,
            delay=1.0  # ì„œë²„ ë¶€í•˜ ë°©ì§€
        )

        print(f"âœ… {len(documents)}ê°œ ë¬¸ì„œ ìˆ˜ì§‘ ì™„ë£Œ")
        return documents

    def chunk_documents(self, documents: List[Document]) -> List[Document]:
        """
        êµ¬ì¡° ê¸°ë°˜ ë¬¸ì„œ ì²­í‚¹

        Args:
            documents: ì›ë³¸ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸

        Returns:
            ì²­í‚¹ëœ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸
        """
        print("\nâœ‚ï¸ êµ¬ì¡° ê¸°ë°˜ ì²­í‚¹ ì‹œì‘...")

        chunked_docs = []
        code_block_count = 0
        section_count = 0

        for doc in documents:
            # êµ¬ì¡° ê¸°ë°˜ ë¶„í• 
            chunks = self.text_splitter.split_documents([doc])

            for chunk in chunks:
                # ë©”íƒ€ë°ì´í„° í†µê³„ ìˆ˜ì§‘
                if chunk.metadata.get('chunk_type') == 'code':
                    code_block_count += 1
                if chunk.metadata.get('section_title'):
                    section_count += 1

                chunked_docs.append(chunk)

        print(f"âœ… ì²­í‚¹ ì™„ë£Œ:")
        print(f"  - ì´ ì²­í¬: {len(chunked_docs)}ê°œ")
        print(f"  - ì½”ë“œ ë¸”ë¡: {code_block_count}ê°œ")
        print(f"  - ì„¹ì…˜: {section_count}ê°œ")

        return chunked_docs

    def load_to_vector_db(
        self,
        documents: List[Document],
        batch_size: int = 100,
        reset: bool = False
    ) -> int:
        """
        ë¬¸ì„œë¥¼ Vector DBì— ì ì¬

        Args:
            documents: ì ì¬í•  ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸
            batch_size: ë°°ì¹˜ í¬ê¸°
            reset: ê¸°ì¡´ ë°ì´í„° ì‚­ì œ ì—¬ë¶€

        Returns:
            ì ì¬ëœ ë¬¸ì„œ ìˆ˜
        """
        print("\nğŸ’¾ Vector DB ì ì¬ ì‹œì‘...")

        # Vector DB ì´ˆê¸°í™”
        if self.use_docker:
            # Docker ChromaDB í—¬ìŠ¤ì²´í¬
            if not self.vector_db.health_check():
                raise ConnectionError(
                    "ChromaDB ì„œë²„ì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\n"
                    "Docker Compose ì‹¤í–‰: docker-compose up -d chromadb"
                )

        # ë²¡í„°ìŠ¤í† ì–´ ì´ˆê¸°í™”
        self.vector_db.init_vectorstore(reset=reset)

        # ë¬¸ì„œ ì¶”ê°€
        ids = self.vector_db.add_documents(
            documents=documents,
            batch_size=batch_size,
            show_progress=True
        )

        print(f"âœ… {len(ids)}ê°œ ë¬¸ì„œ Vector DB ì ì¬ ì™„ë£Œ")

        # í†µê³„ ì¶œë ¥
        stats = self.vector_db.get_statistics()
        print("\nğŸ“Š Vector DB í†µê³„:")
        for key, value in stats.items():
            print(f"  {key}: {value}")

        return len(ids)

    def run_full_pipeline(
        self,
        urls: List[str] = None,
        max_pages: int = 30,
        reset_db: bool = False
    ) -> Dict[str, Any]:
        """
        ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰

        Args:
            urls: ìˆ˜ì§‘í•  URL ëª©ë¡
            max_pages: ìµœëŒ€ ìˆ˜ì§‘ í˜ì´ì§€ ìˆ˜
            reset_db: ê¸°ì¡´ DB ì´ˆê¸°í™” ì—¬ë¶€

        Returns:
            ì‹¤í–‰ ê²°ê³¼ í†µê³„
        """
        start_time = datetime.now()
        print("=" * 60)
        print("ğŸš€ Vector DB ì´ˆê¸°í™” íŒŒì´í”„ë¼ì¸ ì‹œì‘")
        print("=" * 60)

        try:
            # 1. ë¬¸ì„œ ìˆ˜ì§‘
            documents = self.collect_documents(urls, max_pages)

            if not documents:
                print("âŒ ìˆ˜ì§‘ëœ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
                return {"status": "failed", "error": "No documents collected"}

            # 2. êµ¬ì¡° ê¸°ë°˜ ì²­í‚¹
            chunked_docs = self.chunk_documents(documents)

            # 3. Vector DB ì ì¬
            loaded_count = self.load_to_vector_db(
                chunked_docs,
                batch_size=100,
                reset=reset_db
            )

            # 4. í…ŒìŠ¤íŠ¸ ê²€ìƒ‰
            print("\nğŸ” í…ŒìŠ¤íŠ¸ ê²€ìƒ‰...")
            test_query = "LangChainì—ì„œ ë©”ëª¨ë¦¬ë¥¼ ì‚¬ìš©í•˜ëŠ” ë°©ë²•"
            results = self.vector_db.search(test_query, k=3)

            print(f"ì¿¼ë¦¬: '{test_query}'")
            print(f"ê²€ìƒ‰ ê²°ê³¼: {len(results)}ê°œ ë¬¸ì„œ")

            for i, doc in enumerate(results[:2], 1):
                print(f"\n[ê²°ê³¼ {i}]")
                print(f"  ë‚´ìš©: {doc.page_content[:200]}...")
                print(f"  ë©”íƒ€ë°ì´í„°: {doc.metadata}")

            # ì‹¤í–‰ ì‹œê°„ ê³„ì‚°
            execution_time = (datetime.now() - start_time).total_seconds()

            # ê²°ê³¼ ë°˜í™˜
            result = {
                "status": "success",
                "documents_collected": len(documents),
                "chunks_created": len(chunked_docs),
                "documents_loaded": loaded_count,
                "execution_time": f"{execution_time:.2f}ì´ˆ",
                "vector_db_type": "Docker ChromaDB" if self.use_docker else "Local ChromaDB"
            }

            print("\n" + "=" * 60)
            print("âœ… íŒŒì´í”„ë¼ì¸ ì™„ë£Œ!")
            print("=" * 60)

            for key, value in result.items():
                print(f"  {key}: {value}")

            return result

        except Exception as e:
            print(f"\nâŒ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
            return {"status": "failed", "error": str(e)}


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(
        description="Vector DB ì´ˆê¸°í™” ë° ë°ì´í„° ë¡œë”©"
    )

    parser.add_argument(
        "--docker",
        action="store_true",
        help="Docker ChromaDB ì‚¬ìš© (ê¸°ë³¸: ë¡œì»¬)"
    )

    parser.add_argument(
        "--host",
        type=str,
        default="localhost",
        help="Docker ChromaDB í˜¸ìŠ¤íŠ¸ (ê¸°ë³¸: localhost)"
    )

    parser.add_argument(
        "--port",
        type=int,
        default=8000,
        help="Docker ChromaDB í¬íŠ¸ (ê¸°ë³¸: 8000)"
    )

    parser.add_argument(
        "--collection",
        type=str,
        default="langchain_docs",
        help="ì»¬ë ‰ì…˜ ì´ë¦„ (ê¸°ë³¸: langchain_docs)"
    )

    parser.add_argument(
        "--embedding",
        type=str,
        default="solar-embedding-1-large",
        choices=[
            "solar-embedding-1-large",
            "solar-embedding-1-large-query",
            "solar-embedding-1-large-passage",
            "ko-sbert-multitask"
        ],
        help="ì„ë² ë”© ëª¨ë¸ (ê¸°ë³¸: solar-embedding-1-large)"
    )

    parser.add_argument(
        "--max-pages",
        type=int,
        default=30,
        help="ìµœëŒ€ ìˆ˜ì§‘ í˜ì´ì§€ ìˆ˜ (ê¸°ë³¸: 30)"
    )

    parser.add_argument(
        "--reset",
        action="store_true",
        help="ê¸°ì¡´ ë°ì´í„° ì‚­ì œ í›„ ì¬ìƒì„±"
    )

    parser.add_argument(
        "--test-only",
        action="store_true",
        help="í…ŒìŠ¤íŠ¸ ëª¨ë“œ (5ê°œ ë¬¸ì„œë§Œ)"
    )

    args = parser.parse_args()

    # ì´ˆê¸°í™”
    initializer = VectorDBInitializer(
        use_docker=args.docker,
        docker_host=args.host,
        docker_port=args.port,
        collection_name=args.collection,
        embedding_model=args.embedding
    )

    # í…ŒìŠ¤íŠ¸ ëª¨ë“œ
    if args.test_only:
        print("ğŸ§ª í…ŒìŠ¤íŠ¸ ëª¨ë“œ: 5ê°œ ë¬¸ì„œë§Œ ì²˜ë¦¬")
        urls = LANGCHAIN_URLS[:5]
        max_pages = 5
    else:
        urls = LANGCHAIN_URLS
        max_pages = args.max_pages

    # íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
    result = initializer.run_full_pipeline(
        urls=urls,
        max_pages=max_pages,
        reset_db=args.reset
    )

    # ê²°ê³¼ ì €ì¥ (ì„ íƒì‚¬í•­)
    if result["status"] == "success":
        # ë¡œê·¸ íŒŒì¼ ì €ì¥
        log_dir = Path("./logs")
        log_dir.mkdir(exist_ok=True)

        log_file = log_dir / f"vector_db_init_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"

        with open(log_file, 'w', encoding='utf-8') as f:
            import json
            json.dump(result, f, ensure_ascii=False, indent=2)

        print(f"\nğŸ“ ë¡œê·¸ ì €ì¥: {log_file}")


if __name__ == "__main__":
    main()