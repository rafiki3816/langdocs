#!/usr/bin/env python3
"""
Streamlit ê¸°ë°˜ LangChain RAG ì±—ë´‡ ë°ëª¨ (ëŒ€í™” ë©”ëª¨ë¦¬ ê¸°ëŠ¥ í¬í•¨)
ì—°ì† ì§ˆë¬¸ì„ ì²˜ë¦¬í•  ìˆ˜ ìˆëŠ” í–¥ìƒëœ ë²„ì „
"""

import streamlit as st
from typing import List, Dict, Any, Optional
import sys
import os
from datetime import datetime

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì¶”ê°€
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from llm import get_llm, get_embeddings
from vector_database import VectorDatabase
from retriever import HybridRetriever
from conversation import ConversationManager
import sqlite3

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(
    page_title="LangChain RAG ì±—ë´‡ (ë©”ëª¨ë¦¬ ë²„ì „)",
    page_icon="ğŸ¤–",
    layout="wide",
    initial_sidebar_state="expanded"
)

# CSS ìŠ¤íƒ€ì¼
st.markdown("""
    <style>
    .stChat {
        background-color: #f0f2f6;
        border-radius: 10px;
        padding: 10px;
    }
    .source-card {
        background-color: #e8eaf6;
        padding: 10px;
        border-radius: 5px;
        margin: 5px 0;
        border-left: 3px solid #3f51b5;
    }
    .context-info {
        background-color: #fff3e0;
        padding: 8px;
        border-radius: 5px;
        margin: 5px 0;
        font-size: 0.9em;
    }
    </style>
""", unsafe_allow_html=True)

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if 'messages' not in st.session_state:
    st.session_state.messages = []
if 'llm' not in st.session_state:
    st.session_state.llm = None
if 'embeddings' not in st.session_state:
    st.session_state.embeddings = None
if 'retriever' not in st.session_state:
    st.session_state.retriever = None
if 'conversation_manager' not in st.session_state:
    st.session_state.conversation_manager = None
if 'session_id' not in st.session_state:
    st.session_state.session_id = datetime.now().strftime("%Y%m%d_%H%M%S")
if 'conversation_context' not in st.session_state:
    st.session_state.conversation_context = []

def initialize_system():
    """ì‹œìŠ¤í…œ ì´ˆê¸°í™”"""
    with st.spinner("ğŸ”„ ì‹œìŠ¤í…œì„ ì´ˆê¸°í™”í•˜ëŠ” ì¤‘..."):
        try:
            # LLM ì´ˆê¸°í™”
            st.session_state.llm = get_llm(
                model_name="solar-pro",
                temperature=0.7
            )

            # ì„ë² ë”© ì´ˆê¸°í™”
            st.session_state.embeddings = get_embeddings(
                model_name="solar-embedding-1-large"
            )

            # Retriever ì´ˆê¸°í™”
            st.session_state.retriever = HybridRetriever(
                vector_db_path="./data/chroma_db",
                sqlite_db_path="./data/langchain.db"
            )

            # ëŒ€í™” ê´€ë¦¬ì ì´ˆê¸°í™”
            st.session_state.conversation_manager = ConversationManager(
                llm=st.session_state.llm,
                memory_type="buffer_window",
                window_size=10  # ìµœê·¼ 10ê°œ ëŒ€í™”ë§Œ ê¸°ì–µ
            )

            st.success("âœ… ì‹œìŠ¤í…œì´ ì„±ê³µì ìœ¼ë¡œ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤!")
            return True
        except Exception as e:
            st.error(f"âŒ ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}")
            return False

def format_conversation_context(messages: List[Dict], max_turns: int = 5) -> str:
    """ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ë¥¼ í¬ë§·íŒ…"""
    if not messages:
        return ""

    # ìµœê·¼ max_turns ê°œì˜ ëŒ€í™”ë§Œ ì‚¬ìš©
    recent_messages = messages[-max_turns*2:] if len(messages) > max_turns*2 else messages

    context = "ì´ì „ ëŒ€í™” ë‚´ìš©:\n"
    for msg in recent_messages:
        role = "ì‚¬ìš©ì" if msg["role"] == "user" else "ì–´ì‹œìŠ¤í„´íŠ¸"
        # ê¸´ ë©”ì‹œì§€ëŠ” ì¶•ì•½
        content = msg["content"]
        if len(content) > 200:
            content = content[:200] + "..."
        context += f"{role}: {content}\n"

    return context

def generate_response_with_memory(query: str, context: str, conversation_history: str) -> str:
    """ëŒ€í™” ë©”ëª¨ë¦¬ë¥¼ í¬í•¨í•œ ì‘ë‹µ ìƒì„±"""

    # í”„ë¡¬í”„íŠ¸ êµ¬ì„±
    prompt = f"""ë‹¹ì‹ ì€ LangChain ì „ë¬¸ê°€ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.
ì•„ë˜ ì œê³µëœ ì»¨í…ìŠ¤íŠ¸ì™€ ì´ì „ ëŒ€í™” ë‚´ìš©ì„ ì°¸ê³ í•˜ì—¬ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”.

{conversation_history}

í˜„ì¬ ì»¨í…ìŠ¤íŠ¸:
{context}

í˜„ì¬ ì§ˆë¬¸: {query}

ì§€ì¹¨:
1. ì´ì „ ëŒ€í™” ë§¥ë½ì„ ê³ ë ¤í•˜ì—¬ ë‹µë³€í•˜ì„¸ìš”
2. ì‚¬ìš©ìê°€ "ê·¸ê²ƒ", "ì´ê²ƒ", "ìœ„ì˜" ë“± ì§€ì‹œëŒ€ëª…ì‚¬ë¥¼ ì‚¬ìš©í•˜ë©´ ì´ì „ ëŒ€í™”ì—ì„œ ì–¸ê¸‰ëœ ë‚´ìš©ì„ ì°¸ê³ í•˜ì„¸ìš”
3. ì—°ì†ëœ ì§ˆë¬¸ì¸ ê²½ìš° ì´ì „ ë‹µë³€ì„ ë°”íƒ•ìœ¼ë¡œ ë” ìì„¸íˆ ì„¤ëª…í•˜ì„¸ìš”
4. ì»¨í…ìŠ¤íŠ¸ì— ì—†ëŠ” ë‚´ìš©ì€ ì¶”ë¡ í•˜ì§€ ë§ê³  ëª¨ë¥¸ë‹¤ê³  ë‹µí•˜ì„¸ìš”

ë‹µë³€:"""

    response = st.session_state.llm.invoke(prompt)

    # ëŒ€í™” ê´€ë¦¬ìê°€ ìˆìœ¼ë©´ ëŒ€í™” ê¸°ë¡ ì €ì¥
    if st.session_state.conversation_manager:
        st.session_state.conversation_manager.add_user_message(query)
        st.session_state.conversation_manager.add_assistant_message(response.content if hasattr(response, 'content') else str(response))

    return response.content if hasattr(response, 'content') else str(response)

def clear_conversation():
    """ëŒ€í™” ì´ˆê¸°í™”"""
    st.session_state.messages = []
    st.session_state.conversation_context = []
    if st.session_state.conversation_manager:
        st.session_state.conversation_manager.clear()
    st.session_state.session_id = datetime.now().strftime("%Y%m%d_%H%M%S")

# ì‚¬ì´ë“œë°”
with st.sidebar:
    st.header("âš™ï¸ ì„¤ì •")

    # ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    if st.button("ğŸš€ ì‹œìŠ¤í…œ ì´ˆê¸°í™”", type="primary"):
        initialize_system()

    # ëŒ€í™” ì´ˆê¸°í™”
    if st.button("ğŸ—‘ï¸ ëŒ€í™” ì´ˆê¸°í™”"):
        clear_conversation()
        st.rerun()

    st.divider()

    # ê²€ìƒ‰ ì„¤ì •
    st.subheader("ğŸ” ê²€ìƒ‰ ì„¤ì •")
    search_k = st.slider("ê²€ìƒ‰ ë¬¸ì„œ ìˆ˜", min_value=1, max_value=10, value=5)
    show_sources = st.checkbox("ì¶œì²˜ í‘œì‹œ", value=True)
    show_context = st.checkbox("ì»¨í…ìŠ¤íŠ¸ í‘œì‹œ", value=False)

    st.divider()

    # ë©”ëª¨ë¦¬ ì„¤ì •
    st.subheader("ğŸ§  ë©”ëª¨ë¦¬ ì„¤ì •")
    use_memory = st.checkbox("ëŒ€í™” ë©”ëª¨ë¦¬ ì‚¬ìš©", value=True)
    memory_window = st.slider("ë©”ëª¨ë¦¬ ìœˆë„ìš° í¬ê¸°", min_value=2, max_value=20, value=10)

    st.divider()

    # í†µê³„
    st.subheader("ğŸ“Š í†µê³„")

    # ë¬¸ì„œ ìˆ˜ í™•ì¸
    try:
        conn = sqlite3.connect('./data/langchain.db')
        cursor = conn.cursor()
        cursor.execute("SELECT COUNT(*) FROM documents")
        doc_count = cursor.fetchone()[0]
        conn.close()
        st.metric("ì´ ë¬¸ì„œ ìˆ˜", f"{doc_count}ê°œ")
    except:
        st.metric("ì´ ë¬¸ì„œ ìˆ˜", "N/A")

    st.metric("ëŒ€í™” í„´ ìˆ˜", len(st.session_state.messages) // 2)

    if st.session_state.conversation_manager:
        stats = st.session_state.conversation_manager.get_statistics()
        if stats:
            st.metric("í‰ê·  ì‘ë‹µ ê¸¸ì´", f"{stats.get('avg_assistant_length', 0):.0f}ì")

# ë©”ì¸ í™”ë©´
st.title("ğŸ¤– LangChain RAG ì±—ë´‡ (ë©”ëª¨ë¦¬ ê°•í™” ë²„ì „)")
st.markdown("### ğŸ’¡ ì—°ì† ì§ˆë¬¸ì´ ê°€ëŠ¥í•œ ì§€ëŠ¥í˜• ëŒ€í™” ì‹œìŠ¤í…œ")

# ëŒ€í™” ë©”ëª¨ë¦¬ ìƒíƒœ í‘œì‹œ
if use_memory and st.session_state.conversation_manager:
    with st.expander("ğŸ§  ëŒ€í™” ë©”ëª¨ë¦¬ ìƒíƒœ", expanded=False):
        memory_info = st.session_state.conversation_manager.get_memory_string()
        if memory_info:
            st.text(memory_info)
        else:
            st.info("ì•„ì§ ëŒ€í™” ê¸°ë¡ì´ ì—†ìŠµë‹ˆë‹¤")

# íƒ­ ìƒì„±
tab1, tab2, tab3 = st.tabs(["ğŸ’¬ ì±„íŒ…", "ğŸ“š ì˜ˆì œ ì§ˆë¬¸", "â“ ì‚¬ìš©ë²•"])

with tab1:
    # ì‹œìŠ¤í…œ ì²´í¬
    if not st.session_state.llm:
        st.warning("âš ï¸ ì‹œìŠ¤í…œì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì‚¬ì´ë“œë°”ì—ì„œ 'ì‹œìŠ¤í…œ ì´ˆê¸°í™”' ë²„íŠ¼ì„ í´ë¦­í•˜ì„¸ìš”.")

    # ëŒ€í™” ê¸°ë¡ í‘œì‹œ
    chat_container = st.container()
    with chat_container:
        for message in st.session_state.messages:
            with st.chat_message(message["role"]):
                st.markdown(message["content"])

                # ì†ŒìŠ¤ í‘œì‹œ (ì–´ì‹œìŠ¤í„´íŠ¸ ë©”ì‹œì§€ë§Œ)
                if message["role"] == "assistant" and "sources" in message and show_sources:
                    with st.expander("ğŸ“š ì°¸ê³  ë¬¸ì„œ", expanded=False):
                        for source in message.get("sources", []):
                            st.markdown(f"- {source}")

    # ì…ë ¥ ì°½
    if prompt := st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”... (ì—°ì† ì§ˆë¬¸ ê°€ëŠ¥)"):
        if not st.session_state.llm:
            st.error("âŒ ë¨¼ì € ì‹œìŠ¤í…œì„ ì´ˆê¸°í™”í•´ì£¼ì„¸ìš”!")
        else:
            # ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€
            st.session_state.messages.append({"role": "user", "content": prompt})

            # ì‚¬ìš©ì ë©”ì‹œì§€ í‘œì‹œ
            with st.chat_message("user"):
                st.markdown(prompt)

            # ì–´ì‹œìŠ¤í„´íŠ¸ ì‘ë‹µ ìƒì„±
            with st.chat_message("assistant"):
                with st.spinner("ìƒê° ì¤‘..."):
                    try:
                        # ë¬¸ì„œ ê²€ìƒ‰
                        docs = []
                        if st.session_state.retriever:
                            docs = st.session_state.retriever.search(prompt, k=search_k)

                        # ì»¨í…ìŠ¤íŠ¸ ìƒì„±
                        context = ""
                        sources = []

                        if docs:
                            context = "\n\n".join([doc.page_content[:500] for doc in docs])

                            # ì†ŒìŠ¤ ì •ë³´ ìˆ˜ì§‘
                            for doc in docs[:3]:
                                title = doc.metadata.get('title', 'Unknown')
                                url = doc.metadata.get('url', '#')
                                sources.append(f"[{title}]({url})")

                        # ëŒ€í™” íˆìŠ¤í† ë¦¬ ì¤€ë¹„
                        conversation_history = ""
                        if use_memory:
                            conversation_history = format_conversation_context(
                                st.session_state.messages[:-1],  # í˜„ì¬ ë©”ì‹œì§€ ì œì™¸
                                max_turns=memory_window
                            )

                        # ì‘ë‹µ ìƒì„±
                        if context or conversation_history:
                            response = generate_response_with_memory(
                                prompt,
                                context,
                                conversation_history
                            )
                        else:
                            # ì»¨í…ìŠ¤íŠ¸ê°€ ì—†ìœ¼ë©´ ì¼ë°˜ ì‘ë‹µ
                            response = st.session_state.llm.invoke(
                                f"LangChain ì „ë¬¸ê°€ë¡œì„œ ë‹µë³€í•´ì£¼ì„¸ìš”: {prompt}"
                            )
                            response = response.content if hasattr(response, 'content') else str(response)

                        # ì‘ë‹µ í‘œì‹œ
                        st.markdown(response)

                        # ì»¨í…ìŠ¤íŠ¸ ì •ë³´ í‘œì‹œ
                        if show_context and context:
                            st.markdown(
                                f'<div class="context-info">ğŸ“„ {len(docs)}ê°œ ë¬¸ì„œ ì°¸ì¡°</div>',
                                unsafe_allow_html=True
                            )

                        # ì–´ì‹œìŠ¤í„´íŠ¸ ë©”ì‹œì§€ ì €ì¥
                        message_data = {"role": "assistant", "content": response}
                        if sources:
                            message_data["sources"] = sources
                        st.session_state.messages.append(message_data)

                    except Exception as e:
                        error_msg = f"âŒ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
                        st.error(error_msg)
                        st.session_state.messages.append({"role": "assistant", "content": error_msg})

with tab2:
    st.header("ğŸ“š ì—°ì† ì§ˆë¬¸ ì˜ˆì œ")

    st.markdown("""
    ### ê¸°ë³¸ ì§ˆë¬¸ í›„ ì—°ì† ì§ˆë¬¸ ì˜ˆì œ:

    **ì²« ë²ˆì§¸ ì§ˆë¬¸:**
    - "LangChainì´ë€ ë¬´ì—‡ì¸ê°€ìš”?"

    **ì—°ì† ì§ˆë¬¸ë“¤:**
    - "ê·¸ê²ƒì˜ ì£¼ìš” êµ¬ì„± ìš”ì†ŒëŠ” ë¬´ì—‡ì¸ê°€ìš”?"
    - "ë” ìì„¸íˆ ì„¤ëª…í•´ì£¼ì„¸ìš”"
    - "ì˜ˆì œ ì½”ë“œë¥¼ ë³´ì—¬ì£¼ì„¸ìš”"
    - "Agentì— ëŒ€í•´ì„œë„ ì„¤ëª…í•´ì£¼ì„¸ìš”"
    - "ë°©ê¸ˆ ì„¤ëª…í•œ Agentì™€ Toolì˜ ì°¨ì´ëŠ”?"

    ### RAG ê´€ë ¨ ì—°ì† ì§ˆë¬¸:

    **ì²« ë²ˆì§¸ ì§ˆë¬¸:**
    - "RAG ì‹œìŠ¤í…œì˜ ì‘ë™ ì›ë¦¬ë¥¼ ì„¤ëª…í•´ì£¼ì„¸ìš”"

    **ì—°ì† ì§ˆë¬¸ë“¤:**
    - "ê·¸ë ‡ë‹¤ë©´ ë²¡í„° ê²€ìƒ‰ì€ ì–´ë–»ê²Œ ì‘ë™í•˜ë‚˜ìš”?"
    - "ì„ë² ë”©ì€ ì–´ë–»ê²Œ ìƒì„±ë˜ë‚˜ìš”?"
    - "ChromaDBëŠ” ë­”ê°€ìš”?"
    - "ë‹¤ë¥¸ ë²¡í„° DBë„ ì‚¬ìš©í•  ìˆ˜ ìˆë‚˜ìš”?"

    ### Memory ê´€ë ¨ ì—°ì† ì§ˆë¬¸:

    **ì²« ë²ˆì§¸ ì§ˆë¬¸:**
    - "LangChainì˜ ë©”ëª¨ë¦¬ ì¢…ë¥˜ë¥¼ ì„¤ëª…í•´ì£¼ì„¸ìš”"

    **ì—°ì† ì§ˆë¬¸ë“¤:**
    - "ConversationBufferMemoryëŠ” ë­”ê°€ìš”?"
    - "ê·¸ê²ƒê³¼ ConversationSummaryMemoryì˜ ì°¨ì´ëŠ”?"
    - "ì–¸ì œ ì–´ë–¤ ê²ƒì„ ì‚¬ìš©í•´ì•¼ í•˜ë‚˜ìš”?"
    - "ì½”ë“œ ì˜ˆì œë¥¼ ë³´ì—¬ì£¼ì„¸ìš”"
    """)

    if st.button("ğŸ”„ ì˜ˆì œ ì§ˆë¬¸ìœ¼ë¡œ ì‹œì‘í•˜ê¸°"):
        st.session_state.messages = []
        st.session_state.messages.append({"role": "user", "content": "LangChainì´ë€ ë¬´ì—‡ì¸ê°€ìš”?"})
        st.rerun()

with tab3:
    st.header("â“ ì‚¬ìš©ë²•")

    st.markdown("""
    ### ğŸš€ ì‹œì‘í•˜ê¸°

    1. **ì‹œìŠ¤í…œ ì´ˆê¸°í™”**: ì‚¬ì´ë“œë°”ì—ì„œ 'ì‹œìŠ¤í…œ ì´ˆê¸°í™”' ë²„íŠ¼ í´ë¦­
    2. **ì§ˆë¬¸í•˜ê¸°**: ì±„íŒ… ì…ë ¥ì°½ì— ì§ˆë¬¸ ì…ë ¥
    3. **ì—°ì† ì§ˆë¬¸**: ì´ì „ ëŒ€í™”ë¥¼ ì°¸ì¡°í•˜ì—¬ ì¶”ê°€ ì§ˆë¬¸ ê°€ëŠ¥

    ### ğŸ’¡ ì—°ì† ì§ˆë¬¸ íŒ

    - **ì§€ì‹œëŒ€ëª…ì‚¬ ì‚¬ìš©**: "ê·¸ê²ƒ", "ì´ê²ƒ", "ìœ„ì˜" ë“±ì„ ì‚¬ìš©í•˜ì—¬ ì´ì „ ë‚´ìš© ì°¸ì¡°
    - **ì¶”ê°€ ì„¤ëª… ìš”ì²­**: "ë” ìì„¸íˆ", "ì˜ˆì œ í¬í•¨í•´ì„œ" ë“±ìœ¼ë¡œ í™•ì¥ëœ ë‹µë³€ ìš”ì²­
    - **ê´€ë ¨ ì£¼ì œ ì „í™˜**: ì´ì „ ë‹µë³€ê³¼ ê´€ë ¨ëœ ìƒˆë¡œìš´ ì£¼ì œë¡œ ìì—°ìŠ¤ëŸ½ê²Œ ì „í™˜

    ### âš™ï¸ ì„¤ì • ì˜µì…˜

    - **ëŒ€í™” ë©”ëª¨ë¦¬ ì‚¬ìš©**: ì´ì „ ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ë¥¼ ê¸°ì–µ
    - **ë©”ëª¨ë¦¬ ìœˆë„ìš°**: ê¸°ì–µí•  ëŒ€í™” í„´ ìˆ˜ ì„¤ì •
    - **ê²€ìƒ‰ ë¬¸ì„œ ìˆ˜**: ê° ì§ˆë¬¸ì— ëŒ€í•´ ê²€ìƒ‰í•  ë¬¸ì„œ ê°œìˆ˜
    - **ì¶œì²˜ í‘œì‹œ**: ë‹µë³€ì˜ ê·¼ê±°ê°€ ëœ ë¬¸ì„œ í‘œì‹œ

    ### ğŸ“Š í˜„ì¬ ìƒíƒœ

    - ì´ ë¬¸ì„œ: 63ê°œ (LangChain ê³µì‹ ë¬¸ì„œ)
    - ì¹´í…Œê³ ë¦¬: 12ê°œ ì£¼ì œ
    - ë©”ëª¨ë¦¬ íƒ€ì…: ConversationBufferWindowMemory
    """)

# í‘¸í„°
st.divider()
st.markdown(
    f"ğŸ¤– LangChain RAG ì±—ë´‡ v2.0 | ì„¸ì…˜ ID: {st.session_state.session_id} | "
    f"ëŒ€í™” ë©”ëª¨ë¦¬: {'í™œì„±' if use_memory else 'ë¹„í™œì„±'}"
)