#!/usr/bin/env python3
"""
ì„±ëŠ¥ í‰ê°€ ìŠ¤í¬ë¦½íŠ¸
"""

import sys
import os
import time
import json
from datetime import datetime

sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from llm import get_llm
from retriever import HybridRetriever
from evaluator import RAGEvaluator

def run_evaluation():
    """í‰ê°€ ì‹¤í–‰"""

    print("="*60)
    print("ğŸ¯ LangChain RAG ì‹œìŠ¤í…œ ì„±ëŠ¥ í‰ê°€")
    print("="*60)

    # ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    print("\nğŸ“‹ ì´ˆê¸°í™” ì¤‘...")
    llm = get_llm(model="solar-pro", temperature=0.3)
    retriever = HybridRetriever()
    evaluator = RAGEvaluator()  # RAGEvaluatorëŠ” ë§¤ê°œë³€ìˆ˜ ì—†ì´ ì´ˆê¸°í™”

    # í‰ê°€ìš© í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤
    test_cases = [
        {
            "question": "LangChainì´ë€ ë¬´ì—‡ì¸ê°€ìš”?",
            "expected_keywords": ["í”„ë ˆì„ì›Œí¬", "LLM", "ì• í”Œë¦¬ì¼€ì´ì…˜"],
            "category": "ê¸°ì´ˆ"
        },
        {
            "question": "RAG ì‹œìŠ¤í…œì˜ êµ¬ì„± ìš”ì†ŒëŠ”?",
            "expected_keywords": ["ê²€ìƒ‰", "ìƒì„±", "ì„ë² ë”©", "ë²¡í„°"],
            "category": "ì¤‘ê¸‰"
        },
        {
            "question": "Agentì˜ ì—­í• ì€?",
            "expected_keywords": ["ì˜ì‚¬ê²°ì •", "ë„êµ¬", "ì‹¤í–‰", "ê³„íš"],
            "category": "ì¤‘ê¸‰"
        },
        {
            "question": "ConversationBufferMemoryì™€ ConversationSummaryMemoryì˜ ì°¨ì´ì ì€?",
            "expected_keywords": ["ì „ì²´ ì €ì¥", "ìš”ì•½", "í† í°", "ë©”ëª¨ë¦¬"],
            "category": "ê³ ê¸‰"
        },
        {
            "question": "LCELì˜ ì¥ì ì€?",
            "expected_keywords": ["ì²´ì¸", "êµ¬ì„±", "íŒŒì´í”„", "ì„ ì–¸ì "],
            "category": "ê³ ê¸‰"
        }
    ]

    print(f"ğŸ“ {len(test_cases)}ê°œ ì¼€ì´ìŠ¤ í‰ê°€ ì‹œì‘\n")

    results = []

    for i, test_case in enumerate(test_cases, 1):
        print(f"[{i}/{len(test_cases)}] {test_case['category']}: {test_case['question'][:30]}...")

        start_time = time.time()

        # ë¬¸ì„œ ê²€ìƒ‰
        search_results = retriever.hybrid_search(test_case["question"], k=5)
        docs = [result.document for result in search_results] if search_results else []

        # ì»¨í…ìŠ¤íŠ¸ ìƒì„±
        context = "\n".join([doc.page_content[:500] for doc in docs]) if docs else ""

        # ë‹µë³€ ìƒì„±
        prompt = f"""ì»¨í…ìŠ¤íŠ¸: {context}
ì§ˆë¬¸: {test_case['question']}
ë‹µë³€:"""

        response = llm.invoke(prompt)
        answer = response.content if hasattr(response, 'content') else str(response)

        response_time = time.time() - start_time

        # í‰ê°€ ë©”íŠ¸ë¦­ ê³„ì‚°
        keyword_matches = sum(1 for keyword in test_case["expected_keywords"]
                            if keyword.lower() in answer.lower())
        keyword_coverage = keyword_matches / len(test_case["expected_keywords"])

        # ê²°ê³¼ ì €ì¥
        result = {
            "question": test_case["question"],
            "category": test_case["category"],
            "answer_length": len(answer),
            "response_time": response_time,
            "num_docs": len(docs),
            "keyword_coverage": keyword_coverage,
            "keyword_matches": f"{keyword_matches}/{len(test_case['expected_keywords'])}"
        }
        results.append(result)

        print(f"  â±ï¸ ì‹œê°„: {response_time:.2f}ì´ˆ")
        print(f"  ğŸ“Š í‚¤ì›Œë“œ ë§¤ì¹­: {result['keyword_matches']}")
        print(f"  ğŸ“ˆ ì»¤ë²„ë¦¬ì§€: {keyword_coverage:.0%}\n")

    # ì „ì²´ í†µê³„ ê³„ì‚°
    print("="*60)
    print("ğŸ“Š í‰ê°€ ê²°ê³¼ ìš”ì•½")
    print("="*60)

    avg_time = sum(r["response_time"] for r in results) / len(results)
    avg_coverage = sum(r["keyword_coverage"] for r in results) / len(results)
    avg_docs = sum(r["num_docs"] for r in results) / len(results)
    avg_length = sum(r["answer_length"] for r in results) / len(results)

    print(f"\nâœ… í‰ê·  ì‘ë‹µ ì‹œê°„: {avg_time:.2f}ì´ˆ")
    print(f"âœ… í‰ê·  í‚¤ì›Œë“œ ì»¤ë²„ë¦¬ì§€: {avg_coverage:.0%}")
    print(f"âœ… í‰ê·  ì°¸ì¡° ë¬¸ì„œ ìˆ˜: {avg_docs:.1f}ê°œ")
    print(f"âœ… í‰ê·  ë‹µë³€ ê¸¸ì´: {avg_length:.0f}ì")

    # ì¹´í…Œê³ ë¦¬ë³„ ë¶„ì„
    print("\nğŸ“ˆ ì¹´í…Œê³ ë¦¬ë³„ ì„±ëŠ¥:")
    for category in ["ê¸°ì´ˆ", "ì¤‘ê¸‰", "ê³ ê¸‰"]:
        cat_results = [r for r in results if r["category"] == category]
        if cat_results:
            cat_coverage = sum(r["keyword_coverage"] for r in cat_results) / len(cat_results)
            cat_time = sum(r["response_time"] for r in cat_results) / len(cat_results)
            print(f"  {category}: ì»¤ë²„ë¦¬ì§€ {cat_coverage:.0%}, ì‘ë‹µì‹œê°„ {cat_time:.2f}ì´ˆ")

    # ì„±ëŠ¥ ë“±ê¸‰ íŒì •
    print("\nğŸ† ì‹œìŠ¤í…œ ì„±ëŠ¥ ë“±ê¸‰:")
    if avg_coverage >= 0.8 and avg_time <= 5:
        grade = "A (ìš°ìˆ˜)"
    elif avg_coverage >= 0.6 and avg_time <= 7:
        grade = "B (ì–‘í˜¸)"
    elif avg_coverage >= 0.4:
        grade = "C (ë³´í†µ)"
    else:
        grade = "D (ê°œì„  í•„ìš”)"

    print(f"  ë“±ê¸‰: {grade}")

    # ê²°ê³¼ ì €ì¥
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    report_file = f"evaluation_report_{timestamp}.json"

    report = {
        "timestamp": timestamp,
        "test_cases": len(test_cases),
        "average_metrics": {
            "response_time": avg_time,
            "keyword_coverage": avg_coverage,
            "num_docs": avg_docs,
            "answer_length": avg_length
        },
        "grade": grade,
        "detailed_results": results
    }

    with open(report_file, 'w', encoding='utf-8') as f:
        json.dump(report, f, ensure_ascii=False, indent=2)

    print(f"\nğŸ’¾ í‰ê°€ ë³´ê³ ì„œ ì €ì¥: {report_file}")

    # ê°œì„  ì œì•ˆ
    print("\nğŸ’¡ ê°œì„  ì œì•ˆ:")
    if avg_time > 5:
        print("  - ì‘ë‹µ ì‹œê°„ì´ ê¸¸ì–´ ìºì‹± ë˜ëŠ” ìµœì í™” í•„ìš”")
    if avg_coverage < 0.7:
        print("  - í‚¤ì›Œë“œ ì»¤ë²„ë¦¬ì§€ê°€ ë‚®ì•„ ë¬¸ì„œ í’ˆì§ˆ ê°œì„  í•„ìš”")
    if avg_docs < 3:
        print("  - ì°¸ì¡° ë¬¸ì„œê°€ ì ì–´ ë” ë§ì€ ë¬¸ì„œ ìˆ˜ì§‘ í•„ìš”")

    return report

if __name__ == "__main__":
    run_evaluation()